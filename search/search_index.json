{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>Welcome to NHT Labs.</p> <p>Info</p> <p>Estimated time to complete the labs is as follows:</p> <ul> <li>DIY Foundation - 60 minutes</li> <li>Prism Central - 30 minutes</li> <li>XRAY - 60 minutes</li> </ul>"},{"location":"#whats-new","title":"What's New","text":"<ul> <li>Workshop uses for the following software versions:</li> <li>AOS 6.7.1</li> <li>Prism Central pc.2023.x.x</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"<ul> <li>DIY Foundation</li> <li>Deploying Prism Central</li> <li>XRAY Benchmark Testing (Peak Performance Microbenchmark)</li> </ul>"},{"location":"#initial-setup","title":"Initial Setup","text":"<ul> <li>Take note of the Passwords being used from you RX reservation details</li> <li>Log into your virtual desktops (connection info below)</li> <li>Login to Global Protect VPN if you have access</li> </ul>"},{"location":"#cluster-assignment","title":"Cluster Assignment","text":"<p>The instructor will inform the attendees their assigned clusters.</p> <p>Note</p> <p>If these are Single Node Clusters (SNCs) pay close attention on the networking part. The SNCs are setup and configured differently to the 3 or 4 node clusters</p>"},{"location":"#environment-details","title":"Environment Details","text":"<p>Nutanix Workshops are intended to be run in the Nutanix Hosted POC environment. Your cluster will be provisioned with all necessary images,networks, and VMs required to complete the exercises.</p>"},{"location":"#networking","title":"Networking","text":"<p>As we are able to provide three/four node clusters and single node clusters in the HPOC environment, we need to describe each sort of cluster separately. The clusters are setup and configured differently.</p>"},{"location":"#credentials","title":"Credentials","text":"<p>Note</p> <p>The Cluster Password is unique to each cluster and will be provided by the leader of the Workshop.</p>"},{"location":"#access-instructions","title":"Access Instructions","text":"<p>Check your HPOC cluster reservation email for details.</p>"},{"location":"diyfoundation/diyfoundation/","title":"Overview","text":"<p>Info</p> <p>Estimated time to complete: 60 Minutes</p> <p>Foundation is used to automate the installation of the hypervisor and Controller VM on one or more nodes. In this exercise you will practice imaging a physical cluster with Foundation. In order to keep the lab self-contained, you will create a single node cluster on which you will deploy your Foundation VM. That Foundation instance will be used to image and create a cluster from the remaining 3 nodes in the Block.</p> <p>Warning</p> <pre><code>  In following steps, you should replace xx part of the IP octet with your assigned cluster ID\n</code></pre>"},{"location":"diyfoundation/diyfoundation/#diy-environment","title":"DIY Environment","text":"<p>A Hosted POC reservation provides a fully imaged cluster consisting of 4 nodes. To keep the lab self-contained within a single, physical block, you will:</p> <ul> <li>Destroy the existing cluster</li> <li>Confirm the number of SSDs</li> <li>Create a single node cluster using Node D</li> <li>Install the Foundation VM on Node D</li> <li>Use Foundation VM to image Nodes A, B, and C and create a 3 node     cluster</li> </ul>"},{"location":"diyfoundation/diyfoundation/#destroy-the-existing-cluster","title":"Destroy the existing cluster","text":"<ol> <li> <p>Using an SSH client, connect to the Node D CVM IP <code>10.42.xx.32</code> in your assigned block using the following credentials:</p> <ul> <li>Username - nutanix</li> <li>Password - check password in RX</li> </ul> </li> </ol> ssh -l nutanix 10.42.xx.32   # Check password in RX"},{"location":"diyfoundation/diyfoundation/#_1","title":"Foundation","text":"<p>Note</p> <p>Occasionally, Node D is not deployed with .32 as the last octet. Make sure you check and confirm the correct IP address for Node D in your RX reservation.</p> <p>Execute the following commands to power off any running VMs on the cluster, stop cluster services, and destroy the existing cluster:</p> cluster stop# Enter 'I agree' when prompted to proceedcluster stopped"},{"location":"diyfoundation/diyfoundation/#_2","title":"Foundation","text":"cluster destroy# Enter 'Y' when prompted to proceedcluster destroyed <p>Once you have made sure that all VMs and services are stopped, you can proceed to the next steps. </p>"},{"location":"diyfoundation/diyfoundation/#confirm-the-number-of-ssds","title":"Confirm the Number of SSDs","text":"<p>In this section we will confirm the number of SSDs in your node D. This will determine which command we will use in the next section.</p> <p>It is likely that all nodes in HPOC cluster will have similar SSD and HDD combination.</p> <p>Login to the CVM to find out the SSD configuration details</p> <p>Execute the <code>lsscsi</code> command</p> lsscsi# Example output here[0:0:0:0]    disk    ATA      INTEL SSDSC2BX80 0140  /dev/sda  # &lt;&lt; SSD 1[0:0:1:0]    disk    ATA      INTEL SSDSC2BX80 0140  /dev/sdb  # &lt;&lt; SSD 2[0:0:2:0]    disk    ATA      ST91000640NS     SN03  /dev/sdc [0:0:3:0]    disk    ATA      ST91000640NS     SN03  /dev/sdd [0:0:4:0]    disk    ATA      ST91000640NS     SN03  /dev/sde [0:0:5:0]    disk    ATA      ST91000640NS     SN03  /dev/sdf [2:0:0:0]    cd/dvd  QEMU     QEMU DVD-ROM     2.5+  /dev/sr0this output shows that your node D has 2 SSDs"},{"location":"diyfoundation/diyfoundation/#_3","title":"Foundation","text":"lsscsi# Example output here[0:0:0:0]    disk    ATA      INTEL SSDSC2BX80 0140  /dev/sda  # &lt;&lt; SSD 1[0:0:2:0]    disk    ATA      ST91000640NS     SN03  /dev/sdc [0:0:3:0]    disk    ATA      ST91000640NS     SN03  /dev/sdd [0:0:4:0]    disk    ATA      ST91000640NS     SN03  /dev/sde [0:0:5:0]    disk    ATA      ST91000640NS     SN03  /dev/sdf [2:0:0:0]    cd/dvd  QEMU     QEMU DVD-ROM     2.5+  /dev/sr0this output shows that your node D has 1 SSD <p>After confirming the number of SSDs choose the appropriate cluster formation script in the next section.</p>"},{"location":"diyfoundation/diyfoundation/#create-node-d-cluster","title":"Create Node D Cluster","text":"<ol> <li>Remaining in SSH client, access Node-D CVM and execute following commands</li> </ol> ssh -l nutanix 10.42.xx.32   # Check password in RX"},{"location":"diyfoundation/diyfoundation/#_4","title":"Foundation","text":"<ol> <li> <p>Confirm if your hardware nodes have 1 or more SSD. 2 SSDs are required to privide RF2 redundancy factor in a Nutanix cluster.</p> <p>For the purpose of our lab, since we are creating 1 node cluster, we are good to have RF1 as a redundancy factor.</p> 1 SSD Node2 SSDs NodeAll SSDs Node <pre><code>cluster -s 10.42.xx.32 --redundancy_factor=1 create # Enter 'Y' when prompted to proceed\n</code></pre> <pre><code>cluster -s 10.42.xx.32 create # Enter 'Y' when prompted to proceed\n</code></pre> <pre><code>cluster -s 10.42.xx.32 create # Enter 'Y' when prompted to proceed\n</code></pre> </li> <li> <p>After the single node cluster is formed, run the following commands     to configure it</p> <p>Note</p> <p>The above command will create a <code>cluster</code> from a single node using RF1, offering no redundancy to recover from hardware failure.</p> <p>This configuration is being used for non-production, instructional purposes and should NEVER be used for a customer deployment (unless the hosted application stores two copies of the same data    e.g. Splunk). This should be agreed with the customer.</p> <p>After the <code>cluster</code> is created, Prism will reflect Critical Health status due to lack of redundancy.</p> </li> </ol>"},{"location":"diyfoundation/diyfoundation/#_5","title":"Foundation","text":"ncli cluster edit-params new-name=POCxx-D"},{"location":"diyfoundation/diyfoundation/#_6","title":"Foundation","text":"ncli cluster add-to-name-servers servers=10.42.196.10"},{"location":"diyfoundation/diyfoundation/#_7","title":"Foundation","text":"ncli user reset-password user-name='admin' password=&lt;check password in RX&gt;"},{"location":"diyfoundation/diyfoundation/#install-foundation-vm","title":"Install Foundation VM","text":"<ol> <li> <p>Open <code>https://&lt;Node D CVM IP:9440</code> (https://10.42.xx.32:9440) in your browser and log in with the following credentials:</p> <ul> <li>Username - admin</li> <li>Password - check password in RX</li> </ul> </li> <li> <p>Accept the EULA and Pulse prompts.</p> </li> <li> <p>In Home &gt; Storage &gt; Table &gt; Storage Pool, right click on the default storage pool and click Update, then rename it to SP01</p> </li> <li> <p>Under Storage Container, check if there is a container named Images, if not, Click + Storage Container to create a new container named Images</p> <p></p> </li> <li> <p>Go to configuration  (Settings) page and navigate to Image Configuration, click +Upload Image</p> </li> <li> <p>Fill out the following fields and click Save:</p> <ul> <li>Name - Foundation</li> <li>Image Type - Disk</li> <li>Storage Container Images</li> <li>Select From URL</li> <li>Image Source - <code>http://10.42.194.11/images/Foundation/Foundation_VM-5.5-disk-0.qcow2</code></li> </ul> <p>Note</p> <p>At the time of writing this lab, Foundation 5.5 is the latest available version. The URL for the latest Foundation VM QCOW2 image can be downloaded from the Nutanix Portal.</p> <p>Unless otherwise directed by Nutanix Support, always use the latest version of Foundation in a field installation.</p> <p>For the purposes of this lab, the Foundation VM image is stored in a HPOC file server</p> </li> <li> <p>Before creating the VM, we must first create a virtual network to     assign to the Foundation VM. The network will use the Native VLAN     assigned to the physical uplinks for all 4 nodes in the block.</p> </li> <li> <p>In the Prism Element UI click  &gt; Network Configuration &gt; Create Subnet</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>Name - Primary</li> <li>VLAN ID - 0</li> <li>Enable IP address management - leave it unselected</li> </ul> <p></p> </li> <li> <p>Click on Save</p> </li> <li> <p>In Settings &gt; VM &gt; Table and click + Create VM.</p> </li> <li> <p>Fill out the following fields</p> <ul> <li>Name - Foundation</li> <li>vCPU(s) - 2</li> <li>Number of Cores per vCPU - 1</li> <li>Memory - 8 Gi</li> </ul> <p></p> </li> <li> <p>Select + Add New Disk</p> <ul> <li>Operation - Clone from Image Service</li> <li>Image - Foundation</li> <li>Select Add</li> </ul> <p></p> </li> <li> <p>Select Add New NIC</p> <ul> <li>Subnet Name - Primary</li> <li>Select Add</li> </ul> <p></p> <p>Once NIC is added, you should see the NIC configuration in the VM create window as shown here</p> <p></p> </li> <li> <p>Click on Save</p> </li> </ol>"},{"location":"diyfoundation/diyfoundation/#config-foundation-vm","title":"Config Foundation VM","text":"<ol> <li> <p>Select your Foundation VM and click Power on.</p> </li> <li> <p>Once the VM has started, click Launch Console.</p> </li> <li> <p>Once the VM has finished booting, return to Prism element and note     down the IP address of the Foundation VM.</p> </li> <li> <p>Prism Element &gt; VM &gt; Table &gt; Foundation VM &gt; VM NICs</p> <p></p> <p>Warning</p> <pre><code>  The IP address is received from the Primary network default DHCP pool. Your Foundation VM's IP address will be different.\n</code></pre> </li> </ol>"},{"location":"diyfoundation/diyfoundation/#foundation-node-abc-cluster","title":"Foundation Node ABC cluster","text":"<p>Note</p> <p>We will do this section of the lab from your desktop (Windows or Mac) computer. This is the fastest way as remote consoles will be slow.</p> <p>By default, Foundation does not have any AOS or hypervisor images. You can download your desired AOS package from the Nutanix Portal.</p> <p>If downloading the AOS package within the Foundation VM, the <code>.tar.gz</code> package can also be moved to <code>~/foundation/nos</code> rather than uploaded to Foundation through the web UI.</p> <p>To shorten the lab time, we use command line to access foundation VM and download NOS binary to designated folder in it.</p> <ol> <li>Open a terminal in your desktop computer (Putty or Mac Terminal) and ssh to Foundation VM through foundation IP <code>10.42.xx.45</code>. The default password for the Foundation VM can be found on step 7 of the Field Installation Guide.</li> </ol>"},{"location":"diyfoundation/diyfoundation/#_8","title":"Foundation","text":"ssh -l nutanix &lt;Foundation VM IP&gt;  # use default password - ask instructor if you are unaware"},{"location":"diyfoundation/diyfoundation/#_9","title":"Foundation","text":"cd foundationcd noscurl -O http://10.42.194.11/images/AOS/6.7.1/nutanix_installer_package-release-fraser-6.7.1-stable-x86_64.tar.gz <p>Alert</p> <pre><code>When you see 100% finished status, AOS 6.7.1 package has been downloaded to ``~/foundation/nos`` folder.\n</code></pre> <ol> <li> <p>From you desktop computer, open Google Chrome browser and navigate to Foundation VM's IP</p> </li> <li> <p>Access Foundation UI via any browser at <code>http://&lt;Foundation VM IP&gt;</code></p> </li> <li> <p>Fill the following fields:</p> <ul> <li>Select your hardware platform: Autodetect</li> <li>Netmask of Every Host and CVM - 255.255.255.128</li> <li>Gateway of Every Host and CVM - 10.42.xx.1</li> <li>Gateway of Every IPMI - 10.42.xx.1</li> <li>Netmask of Every IPMI - 255.255.255.128</li> <li>Under Double-check this installer's networking step</li> <li>Skip this Validation - selected</li> </ul> <p></p> </li> <li> <p>On the Nodes page, click Add IPMI Nodes Manually</p> <p></p> </li> <li> <p>Fill in block information, fill in the following information:</p> <ul> <li>Number of nodes - 3</li> <li>How should these nodes be reached? - choose I will provide the IPMIs' MAC addresses</li> </ul> </li> <li> <p>Click Add</p> <p></p> <p>Tip</p> <p>Foundation will automatically discover any hosts in the same IPv6 Link Local broadcast domain that is not already part of a cluster. </p> <p>When transferring POC assets in the field, it's not uncommon to receive a cluster that wasn't properly destroyed at the conclusion of the previous POC. In that case, the nodes are already part of existing clusters and will not be discovered. </p> <p>In this lab, we choose manually specify the MAC address instead in order to practice as the real world.</p> <p>Info</p> <p>There are at least 2 methods to get MAC address remotely.</p> <p>Method.1: Identify IPMI MAC Address (BMC MAC address) of Nodes (A, B, and C) by accessing IPMI IP in a browser for each node </p> <p>Method.2 Identify IPMI MAC Address of Nodes (A, B, C) by logging to the AHV hosts with User: root, Password: default for each node and using the following commands: </p> <pre><code>ssh -l root &lt;IP address of Host/Hypervisor&gt;\n</code></pre> <p><pre><code>ipmitool lan print | grep \"MAC Address\" \n</code></pre> <pre><code># output here \nMAC Address             : 0c:c4:7a:3c:c9:ad\n# repeat for nodes B and C for unique IPMI MAC addresses\n</code></pre></p> </li> <li> <p>Access Node A IPMI through IP 10.42.xx.33 with ADMIN/ADMIN</p> <p></p> <p></p> </li> <li> <p>Record your NODE A/B/C BMC MAC address (in above example , it is ac:1f:6b:1e:95:eb )</p> <p>Doing the same with your other 2 nodes B/C, access Node B and C IPMI through IP 10.42.xx.34/35 with ADMIN/ADMIN, record all 3 BMC MAC addresses.</p> </li> <li> <p>Update the Node column to show \"A, B, C\", rather than A, A, A.</p> </li> <li> <p>Click Tools and select Range Autofill from the drop down list</p> </li> <li> <p>Replacing the octet(s) that correspond to your HPOC network, fill out the top row fields with the following details:</p> <ul> <li>IPMI MAC - the three your just recorded down</li> <li>IPMI IP - 10.42.xx.</li> <li>Hypervisor IP - 10.42.xx.25</li> <li>CVM IP - 10.42.xx.29</li> <li>HOSTNAME OF HOST -- POCxx-A</li> </ul> <p></p> </li> <li> <p>Click Next</p> </li> <li> <p>In the Cluster page, fill the following details:</p> <ul> <li>Cluster Name - POCxx-ABC</li> <li>Timezone of Every Hypervisor and CVM - America/Phoenix</li> <li>Cluster Redundancy Factor - RF2</li> <li>Cluster Virtual IP - 10.42.xx.37</li> <li> <p>NTP Servers of Every Hypervisor and CVM:</p> <pre><code>0.pool.ntp.org\n1.pool.ntp.org\n2.pool.ntp.org\n3.pool.ntp.org\n</code></pre> </li> <li> <p>DNS Servers of Every Hypervisor and CVM - 10.42.196.10</p> </li> <li>vRAM Allocation for Every CVM, in Gigabytes - 32</li> </ul> <p></p> </li> <li> <p>Click Next</p> <ul> <li>Select an AOS installer - Select your uploaded (through     command line in previous steps)     nutanix_installer_package-release-.tar.gz* file</li> <li>Arguments to the AOS Installer (Optional) - leave blank</li> </ul> <p></p> </li> <li> <p>Click Next</p> </li> <li> <p>Fill out the following fields and click Next:</p> <ul> <li>Select a hypervisor installer - AHV, AHV installer bundled inside the AOS installer</li> </ul> <p></p> <p>Tip</p> <p>Every AOS release contains a version of AHV bundle with that release.</p> </li> <li> <p>Click Next</p> </li> <li> <p>Enter the existing IPMI credentials as ADMIN and ADMIN for all three nodes. Note that this will be different in the field.</p> <p></p> </li> <li> <p>Click Start</p> </li> <li> <p>Confirm that the installer will be active by clicking on Won't Sleep</p> <p></p> </li> <li> <p>In the Warning of Data Loss Possibility window, click on Ignore and Re-image</p> <p></p> <p>Foundation will run a couple of tests to make sure all the configuration details you have provided are correct and then direct you the installation progress page.</p> </li> <li> <p>Click the Log link to view the realtime log output from your node.</p> <p></p> <p>When all CVMs are ready, Foundation initiates the cluster creation process.</p> </li> <li> <p>Monitor the foundation process until completion</p> <p></p> </li> <li> <p>Once Foundation finishes successully, either click on Click here     link as shown above or open <code>https://&lt;Cluster Virtual IP&gt;:9440</code> (10.42.xx.37)in your browser</p> </li> <li> <p>Log in with the following credentials:</p> <ul> <li>Username - admin</li> <li>Password - Prism Central default password (If you are not familair with this password, it can be found within the Prism Element Web Console Guide, step 5.)</li> </ul> </li> <li> <p>When prompted, Change the Password  to use the same password from RX.</p> </li> <li> <p>Once the password is changed, you can login to Prism Element.</p> <p></p> </li> </ol>"},{"location":"diyfoundation/diyfoundation/#takeaways","title":"Takeaways","text":"<p>You have successfully prepared your environment in a single operation called Foundation:</p> <ul> <li>Installed Hypervisor (AHV) - This can also be ESXi or Hyper-V</li> <li>Installed CVM (AOS)<ul> <li>Distributed File System (Data Plane)</li> <li>Prism Element (Control Plane) </li> </ul> </li> </ul> <p>Now we will proceed to install Prism Central(PC).  PC can manage several Prism Elements akin to cloud managers provided by public cloud providers. </p>"},{"location":"pcdeploy/pcdeploy/","title":"Overview","text":"<p>Info</p> <p>Estimated time to complete: 30 Minutes</p> <p>This lab will introduce Prism Central's(PC) One-Click deploy process</p>"},{"location":"pcdeploy/pcdeploy/#create-primary-and-secondary-networks","title":"Create Primary and Secondary networks","text":"<p>Alert</p> <p>The Primary network is for PC and other VMs deployment, the Secondary network is requried in X-Ray lab</p> <ol> <li> <p>Open <code>https://POCxx-ABC Cluster IP:9440</code> (https://10.42.xx.37:9440) in your browser and log in with the following credentials:</p> <ul> <li>Username - admin</li> <li>Password - check password in RX</li> </ul> </li> <li> <p>In the Prism Element UI click  &gt; Network Configuration &gt; Create Subnet</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>Name - Primary</li> <li>Virtual Switch - vs0</li> <li>VLAN ID - 0</li> <li>Enable IP address management - leave it unselected</li> </ul> </li> <li> <p>Click Save</p> </li> <li> <p>Create the second network by clicking on + Create Subnet with     the following details:</p> <ul> <li>Name - Secondary</li> <li>Virtual Switch - vs0</li> <li>VLAN ID - HPOC Cluster ID 1 (e.g. for PHX-POC079,     VLAN ID would be 791)</li> <li>Enable IP address management - leave it unselected</li> </ul> </li> <li> <p>Click Save</p> </li> <li> <p>You should see two networks as shown here</p> <p></p> </li> </ol>"},{"location":"pcdeploy/pcdeploy/#prism-central-deploy","title":"Prism Central Deploy","text":"<ol> <li> <p>Navigate to Home page and click cluster name POCxx-ABC and provide a cluster data service ip 10.42.xx.38 (found in RX). </p> <p></p> </li> <li> <p>Navigate to Home page and click Register or create new in     Prism Central widget.</p> <p></p> </li> <li> <p>Choose the first Deploy option.</p> <p></p> </li> <li> <p>If no compatiable versions of PC are listed, download the .tar and .json file for latest version from portal.nutanix.com, and then upload the installation binary, before clicking Next.</p> </li> <li> <p>and click Deploy 1-VM PC</p> <p></p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>PC Size - Small - (Up to 2500 VMs)</li> <li>Network - Primary</li> <li>Subnet Mask - 255.255.255.128</li> <li>Gateway - 10.42.XX.1</li> <li>DNS Address(es) - 10.42.196.10</li> <li>NTP Address(es) - 0.pool.ntp.org</li> <li>Select A Container - SelfServiceContainer</li> <li>VM Name - PC</li> <li>IP Address - 10.42.XX.39</li> </ul> <p></p> </li> <li> <p>Click Next</p> </li> <li> <p>For Microservices, click Next.</p> </li> <li> <p>Click Deploy</p> <p>Note</p> <p>The deployment will take about 30 mins, you can go to next lab sessions while waiting. After Prism Central VM is successfully deployed, open <code>https://*PC VM IP*:9440</code> (https://10.42.xx.39:9440) in your browser and log in with the following credentials:</p> </li> <li> <p>When the deployment finishes, browse to your Prism Central IP     address (e.g. 10.42.XX.39) with the following details:</p> <ul> <li>Username - admin</li> <li>Password - default with capital N</li> <li>change password to - check password in RX</li> </ul> </li> <li> <p>Test if you can login Prism Central with the new password</p> </li> <li> <p>Accept EULA if displayed</p> </li> </ol>"},{"location":"pcdeploy/pcdeploy/#prism-central-registration","title":"Prism Central Registration","text":"<ol> <li> <p>Go back to POCxx-ABC Cluster (https://10.42.xx.37:9440)</p> </li> <li> <p>Click Register or create new in Prism Central widget. </p> <p></p> </li> <li> <p>Choose the second Connect option. </p> <p></p> </li> <li> <p>Click Next </p> <p></p> </li> <li> <p>Fill out the following fields, leave others as default and click Connect:</p> <ul> <li>Prism Central IP - 10.42.xx.39</li> <li>Port - 9440</li> <li>Username - admin</li> <li>Password - check password in RX</li> </ul> <p></p> <p>The Prism Central widget will update to show Conntected .</p> <p></p> </li> </ol> <p>You have successully registered Prism Element to be managed your Prism Central.</p> <p>Note</p> <p>Once the Prism Element registration is complete, several management features on Prism Element will be Read-Only mode but fully available in Prism Central.</p>"},{"location":"taskman/taskman/","title":"Deploying Task Manager","text":"<p>The estimated time to complete this lab is 20 minutes.</p>"},{"location":"taskman/taskman/#overview","title":"Overview","text":"<p>This exercise walks you through importing and launching a Calm blueprint to deploy a simple Task Manager application used in Day2's Flow labs. You do not need to complete this exercise unless directed to do so as staging for another lab.</p>"},{"location":"taskman/taskman/#enabling-app-management","title":"Enabling App Management","text":"<p>Open https://&lt;Prism-Central-IP&gt;:9440/ in a browser and log in.</p> <p>From the navigation bar, select Service &gt; Calm</p> <p>Click Enable.</p> <p></p> <p>Select Enable App Management and click Save.</p> <p>::: note ::: title Note :::</p> <p>Nutanix Calm is a separately licensed product that can be used with Acropolis Starter, Pro, or Ultimate editions. Each Prism Central instance can manage up to 25 VMs for free before additional licensing is required. :::</p> <p></p> <p>You should get verification that Calm is enabling, which will take 5 to 10 minutes.</p> <p></p>"},{"location":"taskman/taskman/#creating-a-project","title":"Creating A Project","text":"<p>Projects are the logical construct that integrate Calm with Nutanix\\'s native Self-Service Portal (SSP) capabilities, allowing an administrator to assign both infrastructure resources and the roles/permissions of Active Directory users/groups to specific Blueprints and Applications.</p> <p>Click default in the project list</p> <p></p> <p>Under Infrastructure, fill out the following fields and click comfirm : - Select which resources you want this project to consume - Nutanix - AHV Cluster - \\&lt;POCxx-ABC&gt; - Under Network, select the Primary and if available, the Secondary networks.</p> <p>Select <code>star</code> for the Primary network to make it the default virtual network for VMs in the default project.</p> <p>Click Save.</p> <p></p>"},{"location":"taskman/taskman/#verifying-the-default-project","title":"Verifying the Default Project","text":"<p>In Prism Central, select Menu &gt; Services &gt; Calm.</p> <p></p> <p>Click  Projects in the left hand toolbar and select the default project.</p> <p>::: note ::: title Note :::</p> <p>Mousing over an icon will display its title. :::</p> <p>Under AHV Cluster verify your assigned cluster is selected from the drop-down list, otherwise select it.</p> <p>Under Network, verify the Primary and Secondary networks are selected and the Primary network is the default. Otherwise, make the selections as shown below.</p> <p></p> <p>If changes were made, click Save.</p>"},{"location":"taskman/taskman/#importing-the-blueprint","title":"Importing the Blueprint","text":"<p>Right-click on <code>this link &lt;TaskManager.json&gt;</code>{.interpreted-text role=\"download\"} and Save Link As... to download the blueprint for the example application used in this exercise.</p> <p>Click  Blueprints in the left hand toolbar to view available Calm blueprints.</p> <p>Click Upload Blueprint and select the TaskManager.json file previously downloaded.</p> <p>Fill out the following fields:</p> <ul> <li>Blueprint Name - Initials-TaskManager</li> <li>Project - default</li> </ul> <p></p> <p>Click Upload.</p> <p>::: note ::: title Note :::</p> <p>If you receive an error trying to upload the blueprint, refresh your browser and try again. :::</p>"},{"location":"taskman/taskman/#configuring-the-blueprint","title":"Configuring the Blueprint","text":"<p>Before you can launch the blueprint, you must first provide specify the information not stored in exported Calm blueprints, including credentials.</p> <p>In the Application Profile pane on the right, fill out the following field:</p> <ul> <li>Mysql_password - nutanix/4u</li> </ul> <p></p> <p>Select the WinClient service and in the pane on the right, under the VM tab, ensure the Image is set to the Windows10 disk image as shown below.</p> <p></p> <p>Under Network Adapters (NICs), ensure that NIC 1 is set to Primary as shown below.</p> <p></p> <p>Select the WebServer, HAProxy, and MySQL services and ensure each has NIC 1 set to Primary.</p> <p></p> <p>Click Save.</p> <p></p> <p>Click Credentials.</p> <p></p> <p>Expand the CENTOS credential by clicking its name. Copy and paste the following key into the SSH Private Key field:</p> <pre><code>-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAii7qFDhVadLx5lULAG/ooCUTA/ATSmXbArs+GdHxbUWd/bNG\nZCXnaQ2L1mSVVGDxfTbSaTJ3En3tVlMtD2RjZPdhqWESCaoj2kXLYSiNDS9qz3SK\n6h822je/f9O9CzCTrw2XGhnDVwmNraUvO5wmQObCDthTXc72PcBOd6oa4ENsnuY9\nHtiETg29TZXgCYPFXipLBHSZYkBmGgccAeY9dq5ywiywBJLuoSovXkkRJk3cd7Gy\nhCRIwYzqfdgSmiAMYgJLrz/UuLxatPqXts2D8v1xqR9EPNZNzgd4QHK4of1lqsNR\nuz2SxkwqLcXSw0mGcAL8mIwVpzhPzwmENC5OrwIBJQKCAQB++q2WCkCmbtByyrAp\n6ktiukjTL6MGGGhjX/PgYA5IvINX1SvtU0NZnb7FAntiSz7GFrODQyFPQ0jL3bq0\nMrwzRDA6x+cPzMb/7RvBEIGdadfFjbAVaMqfAsul5SpBokKFLxU6lDb2CMdhS67c\n1K2Hv0qKLpHL0vAdEZQ2nFAMWETvVMzl0o1dQmyGzA0GTY8VYdCRsUbwNgvFMvBj\n8T/svzjpASDifa7IXlGaLrXfCH584zt7y+qjJ05O1G0NFslQ9n2wi7F93N8rHxgl\nJDE4OhfyaDyLL1UdBlBpjYPSUbX7D5NExLggWEVFEwx4JRaK6+aDdFDKbSBIidHf\nh45NAoGBANjANRKLBtcxmW4foK5ILTuFkOaowqj+2AIgT1ezCVpErHDFg0bkuvDk\nQVdsAJRX5//luSO30dI0OWWGjgmIUXD7iej0sjAPJjRAv8ai+MYyaLfkdqv1Oj5c\noDC3KjmSdXTuWSYNvarsW+Uf2v7zlZlWesTnpV6gkZH3tX86iuiZAoGBAKM0mKX0\nEjFkJH65Ym7gIED2CUyuFqq4WsCUD2RakpYZyIBKZGr8MRni3I4z6Hqm+rxVW6Dj\nuFGQe5GhgPvO23UG1Y6nm0VkYgZq81TraZc/oMzignSC95w7OsLaLn6qp32Fje1M\nEz2Yn0T3dDcu1twY8OoDuvWx5LFMJ3NoRJaHAoGBAJ4rZP+xj17DVElxBo0EPK7k\n7TKygDYhwDjnJSRSN0HfFg0agmQqXucjGuzEbyAkeN1Um9vLU+xrTHqEyIN/Jqxk\nhztKxzfTtBhK7M84p7M5iq+0jfMau8ykdOVHZAB/odHeXLrnbrr/gVQsAKw1NdDC\nkPCNXP/c9JrzB+c4juEVAoGBAJGPxmp/vTL4c5OebIxnCAKWP6VBUnyWliFhdYME\nrECvNkjoZ2ZWjKhijVw8Il+OAjlFNgwJXzP9Z0qJIAMuHa2QeUfhmFKlo4ku9LOF\n2rdUbNJpKD5m+IRsLX1az4W6zLwPVRHp56WjzFJEfGiRjzMBfOxkMSBSjbLjDm3Z\niUf7AoGBALjvtjapDwlEa5/CFvzOVGFq4L/OJTBEBGx/SA4HUc3TFTtlY2hvTDPZ\ndQr/JBzLBUjCOBVuUuH3uW7hGhW+DnlzrfbfJATaRR8Ht6VU651T+Gbrr8EqNpCP\ngmznERCNf9Kaxl/hlyV5dZBe/2LIK+/jLGNu9EJLoraaCBFshJKF\n-----END RSA PRIVATE KEY-----\n</code></pre> <p>Expand the WIN_VM_CRED credential by clicking its name. Enter nutanix/4u as the Password.</p> <p></p> <p>Click Save.</p> <p>Once the blueprint has been saved, click Back.</p> <p></p>"},{"location":"taskman/taskman/#launching-the-blueprint","title":"Launching the Blueprint","text":"<p>After the credentials have been provided, Publish, Download, and Launch are now available from the toolbar. Click Launch.</p> <p>Fill out the following fields:</p> <ul> <li>Name of the Application - Initials-TaskManager1</li> <li>User_initials - Initials</li> </ul> <p></p> <p>Click Create.</p> <p>You can monitor the status of your application deployment by clicking  Applications and clicking your application\\'s name.</p> <p>Provisioning the complete application will take approximately 15 minutes. Proceed to the next section of the lab while the application is provisioning.</p>"},{"location":"tools_vms/linux_tools_vm/","title":"Linux Tools VM","text":""},{"location":"tools_vms/linux_tools_vm/#overview","title":"Overview","text":"<p>This CentOS VM image will be staged with packages used to support multiple lab exercises.</p> <p>Deploy this VM on your assigned cluster if directed to do so as part of Lab Setup.</p> <p>Caution</p> <pre><code>Only deploy the VM once, it does not need to be cleaned up as part of any lab completion\n</code></pre>"},{"location":"tools_vms/linux_tools_vm/#deploying-centos","title":"Deploying CentOS","text":"<ol> <li> <p>In Prism Central &gt; select Menu&gt; Compute and Storage and VMs.</p> </li> <li> <p>Click on Create VM</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li> <p>Name - Initials-Linux-ToolsVM</p> </li> <li> <p>Description - (Optional) Description for your VM.</p> </li> <li> <p>vCPU(s) - 2</p> </li> <li> <p>Number of Cores per vCPU - 1</p> </li> <li> <p>Memory - 4 GiB</p> </li> <li> <p>Select Attach Disk</p> <ul> <li>Type - DISK</li> <li>Operation - Clone from Image</li> <li>Image - CentOS7.qcow2</li> <li>Select Save</li> </ul> </li> </ul> </li> <li> <p>Select Attach to Subnet</p> <ul> <li>VLAN Name - Primary</li> <li>Select Save</li> </ul> </li> <li> <p>Click Next and Create VM to create the VM.</p> </li> <li> <p>In the list of VMs, select Initials-Linux-ToolsVM </p> </li> <li> <p>From the Actions menu, choose Power On.</p> </li> </ol>"},{"location":"tools_vms/linux_tools_vm/#installing-linux-tools","title":"Installing Linux Tools","text":"<ol> <li> <p>Login to the VM via ssh or Console session, using the following credentials:</p> </li> <li> <p>Install the software needed by running the following commands:</p> <pre><code>yum update -y\nyum install -y ntp ntpdate unzip stress nodejs python-pip s3cmd awscli\nyum install -y bind-utils nmap wget git\nnpm install -g request\nnpm install -g express\n</code></pre> </li> <li> <p>Enable and configure NTP by running the following commands:</p> <pre><code>systemctl start ntpd\nsystemctl enable ntpd\nntpdate -u -s 0.pool.ntp.org 1.pool.ntp.org 2.pool.ntp.org 3.pool.ntp.org\nsystemctl restart ntpd\n</code></pre> </li> <li> <p>Disable the firewall and SELinux by running the following commands:</p> <pre><code>systemctl disable firewalld\nsystemctl stop firewalld\nsetenforce 0\nsed -i 's/enforcing/disabled/g' /etc/selinux/config /etc/selinux/config\n</code></pre> </li> <li> <p>Optional step - Install Python by running the following commands:</p> <p><pre><code>yum -y install python36\npython3.6 -m ensurepip\nyum -y install python36-setuptools\npip install -U pip\npip install boto3\n</code></pre> Now your Linux Tools VM is ready for you to use.</p> </li> </ol>"},{"location":"tools_vms/windows_tools_vm/","title":"Overview","text":"<p>Deploy this Windows 10 VM on your assigned cluster if directed to do so as part of Lab Setup.</p> <pre><code>&lt;strong&gt;&lt;font color=\"red\"&gt;Only deploy the VM once, it does not need to be cleaned up as part of any lab completion.&lt;/font&gt;&lt;/strong&gt;\n</code></pre>"},{"location":"tools_vms/windows_tools_vm/#deploying-tools-vm","title":"Deploying Tools VM","text":"<p>Using an SSH client, connect to the Node A CVM IP \\&lt;10.42.xx.29&gt; in your assigned block using the following credentials:</p> <p>Username - nutanix</p> <p>Password - default</p> <p>Execute the following commands to upload AD image:</p> <pre><code>acli image.create Windows10 container=Images image_type=kDiskImage source_url=https://s3.amazonaws.com/get-ahv-images/Windows10-1709.qcow2\n</code></pre> <p>In Prism Central &gt; select <code>bars</code>{.interpreted-text role=\"fa\"} &gt; Virtual Infrastructure &gt; VMs, and click Create VM.</p> <p>Fill out the following fields:</p> <ul> <li> <p>Name - Initials-Windows-ToolsVM</p> </li> <li> <p>Description - (Optional) Description for your VM.</p> </li> <li> <p>vCPU(s) - 1</p> </li> <li> <p>Number of Cores per vCPU - 2</p> </li> <li> <p>Memory - 4 GiB</p> </li> <li> <p>Select + Add New Disk</p> <p>:   -   Type - DISK     -   Operation - Clone from Image Service     -   Image - Windows10-1709.qcow2     -   Select Add</p> </li> <li> <p>Select Add New NIC</p> <p>:   -   VLAN Name - Secondary     -   Select Add</p> </li> </ul> <p>Click Save to create the VM.</p> <p>Power On the VM.</p> <p>Login to the VM via RDP or Console session, using the following credentials:</p> <ul> <li>Username - NTNXLAB\\Administrator</li> <li>password - nutanix/4u</li> </ul>"},{"location":"xray/xray/","title":"Overview","text":"<p>Info</p> <p>Estimated time to complete: 60 Minutes</p> <p>X-Ray is an automated testing application for virtualized infrastructure solutions. It is capable of running test scenarios end-to-end to evaluate system attributes in real-world use cases. In this exercise you will deploy and configure an X-Ray VM, run X-Ray tests, and analyze results.</p> <p>As X-Ray powers down hosts for tests that evaluate availability and data ntegrity, it is best practice to run the X-Ray VM outside of the target cluster. Additionally, the X-Ray VM itself creates a small amount of storage and CPU overhead that could potentially skew results.</p> <p>In this lab, we will deploy X-Ray VM on POCxx-D, and evalutate cluster POCxx-ABC we just created.</p> <p>For environments where DHCP is unavailable (or there isn't a sufficiently large pool of addresses available), X-Ray supports Link-local or Zero Configuration networking, where the VMs communicate via self-assigned IPv4 addresses. In order to work, all of the VMs (including the X-Ray VM) need to reside on the same Layer 2 network. To use Link-local networking, your X-Ray VM's first NIC (eth0) should be on a network capable of communicating with your cluster. A second NIC (eth1) is added on a network without DHCP.</p>"},{"location":"xray/xray/#create-the-x-ray-vm-image","title":"Create the X-Ray VM image","text":"<p>Open a terminal and SSH to Node-D CVM, enter CVM credentials and execute following commands</p> ssh -l nutanix 10.42.xx.32 # Check password in RX <p>Upload the X-Ray Image</p> acli image.create X-Ray container=Images image_type=kDiskImage  \\   source_url=http://10.42.194.11/images/Xray/4.4.1/xray-4.4.1.qcow2 Image uploaded <p>Warning</p> <pre><code>  Wait until you see that the image upload is complete with a message ``X-Ray: Complete``\n</code></pre> <p>Confirm the presence of X-Ray image by running the following command in the same shell.</p> acli image.list# Output hereImage name  Image type  Image UUID                            Foundation  kDiskImage  c970941a-d583-4640-8e03-9b2ca7336d00  X-Ray       kDiskImage  ac819fab-3fb9-4e85-99fd-97ca3f925ec8  &lt;&lt; here is your X-Ray Image"},{"location":"xray/xray/#configuring-networks","title":"Configuring Networks","text":"<p>For targeting network, we will use the Secondary network VLAN for communication between the X-Ray VM and X-Ray worker VMs. This is accomplished via \"Zero Configuration\" networking, as the 3-node cluster Secondary and 1-node cluster Secondary networks are the same Layer 2 network and there is no DHCP.</p> <p>Now we switch to Prism portal of single node cluster D</p> <ol> <li>Open <code>https://&lt;POCxx-D Cluster IP&gt;:9440</code> (https://10.42.xx.32:9440) in your browser and log in with the following credentials:<ul> <li>Username - admin</li> <li>Password - check password in RX</li> </ul> </li> <li>Click  &gt; Network Configuration &gt; Create Subnet<ul> <li>Name - Secondary</li> <li>Virtual Switch - vs0</li> <li>VLAN ID - HPOC Cluster ID 1 (e.g. for PHX-POC079, VLAN ID would be 791)</li> <li>Enable IP address management - leave it unselected</li> </ul> </li> <li>Click on Save.</li> </ol>"},{"location":"xray/xray/#creating-x-ray-vm","title":"Creating X-Ray VM","text":"<ol> <li> <p>In Prism &gt; VM &gt; Table and click + Create VM.</p> </li> <li> <p>Fill out the following fields and click Save:</p> <ul> <li>Name - X-Ray</li> <li>vCPU(s) - 2</li> <li>Number of Cores per vCPU - 1</li> <li>Memory - 4 GiB</li> <li>Select + Add New Disk<ul> <li>Operation - Clone from Image Service</li> <li>Image - X-Ray</li> <li>Select Add</li> </ul> </li> <li>Select Add New NIC<ul> <li>VLAN Name - Primary</li> <li>Select Add</li> </ul> </li> <li>Select + Add New NIC<ul> <li>VLAN Name - Secondary</li> <li>Select Add</li> </ul> </li> </ul> </li> <li> <p>Select your X-Ray VM and click Power on.</p> <p>Info</p> <p>At the time of writing, X-Ray 4.4.1 is the latest available version. The URL for the latest X-Ray OVA &amp; QCOW2 images can be downloaded from the Nutanix Portal.</p> </li> <li> <p>Once the VM has started, click Launch Console</p> </li> <li> <p>Make sure the VM is booting to console.</p> </li> <li> <p>Your X-Ray VM would have received an IP address from the DHCP server in Primary network</p> </li> <li> <p>Determine the IP address of the NIC (eth0) on the Primary network of the X-Ray VM from Prism Element and note it down</p> <p>Note</p> <p>It is critical that you select the IP address of the network adapter assigned to the Primary network (you can confirm by comparing the MAC address in the VM console to the MAC address shown in Prism).</p> </li> </ol>"},{"location":"xray/xray/#configuring-x-ray","title":"Configuring X-Ray","text":"<ol> <li> <p>Open <code>http://X-RAY-VM-IP</code> (E.g: http://10.42.xx.52) in a browser</p> <p>Warning</p> <pre><code>  Make sure to use the X-RAY-VM-IP that you noted down from the previous section\n</code></pre> </li> <li> <p>Click on Log in with Local Account</p> <p></p> </li> <li> <p>Click on Sign up now in the bottom of the screen</p> </li> <li> <p>Provide the following details</p> <ul> <li>Email - youremail@nutanix.com</li> <li>Password - set it to cluster password</li> </ul> </li> <li> <p>Click on Submit</p> </li> <li> <p>Select I have read and agree to the terms and conditions and click Accept.</p> <p></p> </li> <li> <p>Select Targets from the navigation bar and click Add Target. Fill out the following fields and click Next:</p> <ul> <li>Name - POCxx-ABC</li> <li>Manager Type - Prism</li> <li>Cluster Type - Nutanix</li> <li>Hypervisor - AHV</li> <li>Address - 3-Node Cluster Virtual IP 10.42.xx.37</li> <li>Username - admin</li> <li>Password - you 3 node cluster password</li> <li>Expiration Time - leave blank</li> </ul> <p></p> </li> <li> <p>Click Next</p> </li> <li> <p>Select Secondary under Network and click Next.</p> <p></p> </li> <li> <p>Click Next</p> </li> <li> <p>Under OOB Management Protocol, choose IPMI</p> </li> <li> <p>Review A, B and C node configurations</p> </li> <li> <p>From the drop down menu, choose Fill with Nutanix defaults</p> <p></p> </li> <li> <p>Click Next</p> </li> <li> <p>Click Run Validation.</p> <p></p> </li> <li> <p>Click Check Details to view validation progress.</p> <p></p> </li> <li> <p>Upon successful completion of validation, click Done.</p> <p></p> </li> </ol>"},{"location":"xray/xray/#running-x-ray-tests","title":"Running X-Ray Tests","text":"<p>While X-Ray offers many testing options, we will use Peak Performance Microbenchmark test in this lab.</p> <ol> <li> <p>Select Tests from the navigation bar</p> </li> <li> <p>In the list of tests, find Peak Performance Microbenchmark and click on View &amp; Run Test</p> <p></p> </li> <li> <p>Review the test description</p> </li> <li> <p>Enter the test name as Your Initials - Peak Perforamce Test</p> </li> <li> <p>Confirm your POCxx-ABC as the right target</p> </li> <li> <p>Choose the Default test variant</p> <p></p> </li> <li> <p>Click Run test.</p> <p>Warning</p> <pre><code>  X-Ray can run one test per target at a time. Many tests can be queued for a single target, allowing X-Ray to automatically run through multiple tests without requiring manual intervention. Through automation, X-Ray can drastically decrease the amount of time to conduct a POC.\n</code></pre> </li> <li> <p>Click on View Test</p> </li> <li> <p>You are able to monitor the test progress and results in the Results page</p> </li> <li> <p>Click on In Progress link to see which stage you are at in the test</p> <p></p> </li> <li> <p>You can see the random/sequential read/write tests are coming up soon</p> <p></p> </li> <li> <p>Click on Got it to return to the test Results page</p> </li> <li> <p>As the test runs you will be able to see the test results as shown here</p> <p>Note the minimum and maximum performance numbers all in one screen.</p> <p></p> </li> <li> <p>You are also able to get detailed view of the metrics as a Grafana dashboard, click on the Grafana Dashboard link</p> <p></p> </li> <li> <p>Grafana dashboards presents detailed metrics view</p> <p></p> </li> <li> <p>You are also able to generate reports (in PDF), export Test Results and re-run the tests.</p> <p></p> <p>Tip</p> <p>The graphs are interactive, and you can click and drag to zoom into specific data/times on each individual graph. You can zoom out by clicking Reset Zoom.</p> <p>Each dotted blue line represents an event in the test, such as beginning a workload, powering off a node, etc. Clicking the blue dots will provide information about the event.</p> <p>Clicking the Actions drop down menu provides options to view the detailed log data, export the test results, and generate a PDF report.</p> </li> </ol>"},{"location":"xray/xray/#working-with-x-ray-results","title":"Working with X-Ray Results","text":"<p>As X-Ray is using automation to perform the exact same tests and collect the same metrics on multiple systems/hypervisors, the results can be easily overlaid to compare solutions. In this exercise you will use X-Ray to compare BigData Ingestion test results between Nutanix and a competitor.</p> <p>The BigData Ingestion test compares the speed at which 1TB of sequential data can be written to a single VM on a cluster, as is common in workloads such as Splunk.</p> <ol> <li> <p>Download the following exported X-Ray test result:</p> <p>Competitor + Nutanix Big Data Ingest Results</p> </li> <li> <p>Select Results &gt; Import Test Result Bundle from the navigation bar.</p> </li> <li> <p>Click Choose File and select the Nutanix test results .zip file previously downloaded.</p> </li> <li> <p>Click Upload.</p> <p></p> </li> <li> <p>Once the file successfully uploads, you will see three results as shown here</p> <p></p> </li> <li> <p>Select all 3 BigData Ingestion results and click Create Comparison.</p> <p></p> </li> </ol> <p>The resulting charts show the combined metrics for all three solutions. You are able to see the software and hardware versions of the infrastructure cluster where the test was conducted.</p> <p>By hovering over the graph you can also see point-in-time performance metrics.</p> <p>In this case we can clearly see that the Nutanix solution is able to sustain a higher, and more consistent, rate of write throughput, resulting in a much faster time to complete ingesting the 1TB of data.</p> <p></p>"},{"location":"xray/xray/#exporting-x-ray-results","title":"Exporting X-Ray Results","text":"<ol> <li> <p>To export analysis results for use in proposal documents, etc., </p> </li> <li> <p>Select the results you need in All Results page and click on</p> <p>Create report.</p> <p></p> <p>The results will open and can be printed or exported as PDF.</p> <p>Here is a results file for your reference.</p> <p>X-Ray Export Result PDF</p> </li> <li> <p>Multiple analyses can also be selected to generate a combined report     with the results from multiple tests, this can be extremely useful     for summarizing POC results.</p> <p>Question</p> <pre><code>   Can you explain **why** the Nutanix solution may produce better results than common HCI competitors?\n</code></pre> <p>Tip</p> <p>Check out the OpLog section of the Nutanix Bible</p> </li> </ol>"},{"location":"xray/xray/#takeaways","title":"Takeaways","text":"<ul> <li>X-Ray is a easy to use benchmarking tool to make your life in the field easier</li> <li>X-Ray has many testing scenarios (database, big data, etc) for specific use-cases</li> <li>X-Ray is available as a VM appliance as well as SaaS (tests can be run only on Nutanix HPOC clusters)</li> <li>X-Ray testing parameters can be customised easily to suit yours or your customer's testing requierements</li> </ul>"}]}